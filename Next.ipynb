{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCAhoI9HHGoE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/Sherlock Holmes.txt\", \"r\") as file:\n",
        "    text_data = file.read()"
      ],
      "metadata": {
        "id": "birDSBLJHzro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense,Dropout,Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ],
      "metadata": {
        "id": "KfcOTzPRH4Uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text_data])\n",
        "total_words = len(tokenizer.word_index) + 1\n"
      ],
      "metadata": {
        "id": "GhxO3_bXH8M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_words"
      ],
      "metadata": {
        "id": "Re8qP9fTH-Nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input sequences and targets\n",
        "input_sequences = []\n",
        "for line in text_data.split('\\n'):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)"
      ],
      "metadata": {
        "id": "vO_qker1IAYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad sequences to have equal length\n",
        "max_length = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_length, padding='pre'))"
      ],
      "metadata": {
        "id": "X-jGW5BLICOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split input sequences into input and target\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "y = to_categorical(y, num_classes=total_words)"
      ],
      "metadata": {
        "id": "zYzVdo7HIHYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape,y.shape"
      ],
      "metadata": {
        "id": "dBqguhS7JJzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
        "\n",
        "# Define the optimizer with a specific learning rate\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# Define the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    # Embedding layer to convert words to dense vectors\n",
        "    Embedding(input_dim=total_words, output_dim=300, input_length=max_length-1),\n",
        "\n",
        "    # Bidirectional LSTM layer to capture bidirectional context\n",
        "    Bidirectional(LSTM(units=256, return_sequences=True)),\n",
        "\n",
        "    # Dropout layer to prevent overfitting\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # Bidirectional LSTM layer for deeper context understanding\n",
        "    Bidirectional(LSTM(units=128)),\n",
        "\n",
        "    # Dropout layer for regularization\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # Dense layer with softmax activation for prediction\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with the specified optimizer\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "i__wqDZcIJZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "w_8HeEroITRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X,y,epochs = 50)"
      ],
      "metadata": {
        "id": "f0T6E9JtImsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('nxt_word.h5')\n"
      ],
      "metadata": {
        "id": "4tI361bBInpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'It '\n",
        "for a in range(30):\n",
        "  tok_text = tokenizer.texts_to_sequences([text])[0]\n",
        "  pad_text = pad_sequences([tok_text],17,padding = 'pre')\n",
        "  pred = np.argmax(model.predict(pad_text))\n",
        "  for k,v in tokenizer.word_index.items():\n",
        "    if v == pred:\n",
        "      print(f'Next word after \"{text}\" is: ',k)\n",
        "      text = text + \" \" + k"
      ],
      "metadata": {
        "id": "AP1MSWEPebq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vot59YgAe9SD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}